{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf_8OI34CdXU"
   },
   "source": [
    "<img src=\"images/sutd.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
    "\n",
    "## <center>50.040 Natural Language Processing, Summer 2021<center>\n",
    "    \n",
    "<center>**Due 17 June 2021, 5pm** <center>\n",
    "Mini Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YAN5MA5GCdXU"
   },
   "source": [
    "**Write your student ID and name**\n",
    "\n",
    "\n",
    "### STUDNET ID: 1004335\n",
    "\n",
    "### Name: Dody Senputra\n",
    "\n",
    "### Students with whom you have discussed (if any):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0iF5uWcCdXV"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Language models are very useful for a wide range of applications, e.g., speech recognition and machine translation. Consider a sentence consisting of words $x_1, x_2, …, x_m$, where $m$ is the length of the sentence, the goal of language modeling is to model the probability of the sentence, where $m \\geq 1$, $x_i \\in V $ and $V$ is the vocabulary of the corpus:\n",
    "$$p(x_1, x_2, …, x_m)$$\n",
    "In this project, we are going to explore both statistical language model and neural language model on the [Wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) datasets. Download wikitext-2 word-level data and put it under the ``data`` folder.\n",
    "\n",
    "## Statistical  Language Model\n",
    "\n",
    "A simple way is to view words as independent random variables (i.e., zero-th order Markovian assumption). The joint probability can be written as:\n",
    "$$p(x_1, x_2, …, x_m)=\\prod_{i=1}^m p(x_i)$$\n",
    "However, this model ignores the word order information, to account for which, under the first-order Markovian assumption, the joint probability can be written as:\n",
    "$$p(x_0, x_1, x_2, …, x_{m})= \\prod_{i=1}^{m}p(x_i \\mid x_{i-1})$$\n",
    "Under the second-order Markovian assumption, the joint probability can be written as:\n",
    "$$p(x_{-1}, x_0, x_1, x_2, …, x_{m})= \\prod_{i=1}^{m}p(x_i \\mid x_{i-2}, x_{i-1})$$\n",
    "Similar to what we did in HMM, we will assume that $x_{-1}=START, x_0=START, x_{m} = STOP$ in this definition, where $START, STOP$ are special symbols referring to the start and the end of a sentence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5wC4lqKCdXW"
   },
   "source": [
    "### Parameter estimation\n",
    "\n",
    "Let's use $count(u)$ to denote the number of times the unigram $u$ appears in the corpus, use $count(v, u)$ to denote the number of times the bigram $v, u$ appears in the corpus, and $count(w, v, u)$ the times the trigram $w, v, u$ appears in the corpus, $u \\in V \\cup STOP$ and $w, v \\in V \\cup START$.\n",
    "\n",
    "And the parameters of the unigram, bigram and trigram models can be obtained using maximum likelihood estimation (MLE).\n",
    "\n",
    "- In the unigram model, the parameters can be estimated as: $$p(u) = \\frac {count(u)}{c}$$, where $c$ is the total number of words in the corpus.\n",
    "- In the bigram model, the parameters can be estimated as:\n",
    "$$p(u \\mid v) = \\frac{count(v, u)}{count(v)}$$\n",
    "- In the trigram model, the parameters can be estimated as:\n",
    "$$p(u \\mid w, v) = \\frac{count(w, v, u)}{count(w, v)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7dks8t0CdXW"
   },
   "source": [
    "### Smoothing the parameters\n",
    "#### Add-k Smoothing\n",
    "Note, it is likely that many parameters of bigram and trigram models will be 0 because the relevant bigrams and trigrams involved do not appear in the corpus. If you don't have a way to handle these 0 probabilities, all the sentences that include such bigrams or trigrams will have probabilities of 0.\n",
    "\n",
    "We'll use a Add-k Smoothing method to fix this problem, the smoothed parameters can be estimated as:\n",
    "\\begin{equation}\n",
    "p_{add-k}(u)= \\frac{count(u)+k}{c+k|V^*|}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "p_{add-k}(u \\mid v)= \\frac{count(v, u)+k}{count(v)+k|V^*|}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "p_{add-k}(u \\mid w, v)= \\frac{count(w, v, u)+k}{count(w, v)+k|V^*|}\n",
    "\\end{equation}\n",
    "\n",
    "where $k \\in (0, 1)$ is the parameter of this approach, and $|V^*|$ is the size of the vocabulary $V^*$, here $V^*= V \\cup STOP$. One way to choose the value of $k$ is by\n",
    "optimizing the perplexity of the development set, namely to choose the value that minimizes the perplexity.\n",
    "#### Interpolation\n",
    "There is another way for smoothing which is named as **interpolation**. In interpolation, we always mix the probability estimates from\n",
    "all the n-gram estimators, weighing and combining the trigram, bigram, and unigram counts. In simple linear interpolation, we combine different order n-grams by linearly interpolating all the models. Thus, we estimate the trigram probability $p(w_n|w_{n-2},w_{n-1})$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $\\lambda$:\n",
    "\\begin{align}\n",
    "\\hat{p}(w_n|w_{n-2},w_{n-1}) = \\lambda_1p(w_n|w_{n-2},w_{n-1})+\\lambda_2p(w_n|w_{n-1})+\\lambda_3p(w_n)\n",
    "\\end{align}\n",
    "such that the $\\lambda$s sum to 1:\n",
    "\\begin{align}\n",
    "\\sum_i\\lambda_i=1\n",
    "\\end{align}\n",
    "In addition, $\\lambda_1,\\lambda_2,\\lambda_3\\geq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNVha_8UCdXX"
   },
   "source": [
    "### Perplexity\n",
    "\n",
    "Given a test set $D^{\\prime}$ consisting of sentences $X^{(1)}, X^{(2)}, …, X^{(|D^{\\prime}|)}$, each sentence $X^{(j)}$ consists of words $x_1^{(j)}, x_2^{(j)},…,x_{n_j}^{(j)}$, we can measure the probability of each sentence $X^{(j)}$, and the quality of the language model would be the probability it assigns to the entire set of test sentences, namely:\n",
    "\\begin{equation} \n",
    "\\prod_{j=1}^{|D^{\\prime}|}p(X^{(j)})\n",
    "\\end{equation}\n",
    "Let's define average $log_2$ probability as:\n",
    "\\begin{equation} \n",
    "l=\\frac{1}{c^{\\prime}}\\sum_{j=1}^{|D^{\\prime}|}log_2p(X^{(j)})\n",
    "\\end{equation}\n",
    "$c^{\\prime}$ is the total number of words in the test set, $|D^{\\prime}|$ is the number of sentences. And the perplexity is defined as:\n",
    "\\begin{equation} \n",
    "perplexity=2^{-l}\n",
    "\\end{equation}\n",
    "\n",
    "The lower the perplexity, the better the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/wikitext-2/wiki.train.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    train_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    train_sents = [s for s in train_sents if len(s)>0 and s[0] != '=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', ',', 'it', 'also', 'underwent', 'multiple', 'adjustments', ',', 'such', 'as', 'making', 'the', 'game', 'more', '<unk>', 'for', 'series', 'newcomers', '.', 'character', 'designer', '<unk>', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', ',', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', '.', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', 'the', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'n\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_sents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0vxL-WpCdXX"
   },
   "source": [
    "### Question 1 [code]\n",
    "1. Implement the function **\"compute_ngram\"** that computes n-grams in the corpus.\n",
    " (Do not take the START and STOP symbols into consideration for now.) \n",
    "2. List 10 most frequent unigrams, bigrams and trigrams as well as their counts.(Hint: use the built-in function .most_common in Counter class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOyoT-QiCdXY"
   },
   "outputs": [],
   "source": [
    "def compute_ngram(sents, n):\n",
    "    \"\"\"\n",
    "    Compute n-grams that appear in \"sents\".\n",
    "    param:\n",
    "        sents: list[list[str]] --- list of list of word strings\n",
    "        n: int --- \"n\" gram\n",
    "    return:\n",
    "        ngram_set: set{str} --- a set of n-grams (no duplicate elements)\n",
    "        ngram_dict: dict{ngram: counts} --- a dictionary that maps each ngram to its number occurence in \"sents\";\n",
    "        This dict contains the parameters of our ngram model. E.g. if n=2, ngram_dict={('a','b'):10, ('b','c'):13}\n",
    "\n",
    "        You may need to use \"Counter\", \"tuple\" function here.\n",
    "    \"\"\"\n",
    "    ngram_set = set()\n",
    "    ngram_dict = {}\n",
    "    ### YOUR CODE HERE\n",
    "    for sentence in sents:\n",
    "        for i in range(len(sentence) - n):\n",
    "            data = tuple(sentence[i : i + n])\n",
    "            if data in ngram_dict:\n",
    "                ngram_dict[data] += 1\n",
    "            else:\n",
    "                ngram_dict[data] = 1\n",
    "\n",
    "    ngram_set = list(ngram_dict.keys())\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return ngram_set, ngram_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 497 ms, sys: 13 ms, total: 510 ms\n",
      "Wall time: 510 ms\n",
      "unigram: 28900\n",
      "CPU times: user 814 ms, sys: 25.5 ms, total: 840 ms\n",
      "Wall time: 845 ms\n",
      "bigram: 575544\n",
      "CPU times: user 782 ms, sys: 39.9 ms, total: 822 ms\n",
      "Wall time: 824 ms\n",
      "trigram: 1334673\n"
     ]
    }
   ],
   "source": [
    "%time unigram_set, unigram_dict = compute_ngram(train_sents, 1)\n",
    "print('unigram: %d' %(len(unigram_set)))\n",
    "\n",
    "%time bigram_set, bigram_dict = compute_ngram(train_sents, 2)\n",
    "print('bigram: %d' %(len(bigram_set)))\n",
    "\n",
    "%time trigram_set, trigram_dict = compute_ngram(train_sents, 3)\n",
    "print('trigram: %d' %(len(trigram_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((',', 'and', 'the'), 1393),\n",
       " ((',', '<unk>', ','), 950),\n",
       " (('<unk>', ',', '<unk>'), 900),\n",
       " (('one', 'of', 'the'), 866),\n",
       " (('<unk>', ',', 'and'), 819),\n",
       " (('.', 'however', ','), 775),\n",
       " (('<unk>', '<unk>', ','), 743),\n",
       " (('.', 'in', 'the'), 726),\n",
       " (('.', 'it', 'was'), 698),\n",
       " (('the', 'united', 'states'), 665)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List 10 most frequent unigrams, bigrams and trigrams as well as their counts.\n",
    "### YOUR CODE HERE\n",
    "Counter(unigram_dict).most_common(10)\n",
    "Counter(bigram_dict).most_common(10)\n",
    "Counter(trigram_dict).most_common(10)\n",
    "### END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVpoDgU7CdXb"
   },
   "source": [
    "### Question 2 [code]\n",
    "In this part, we take the START and STOP symbols into consideration. So we need to pad the **train_sents** as described in \"Statistical Language Model\" before we apply \"compute_ngram\" function. For example, given a sentence \"I like NLP\", in a bigram model, we need to pad it as \"START I like NLP STOP\", in a trigram model, we need to pad it as \"START START I like NLP STOP\". For unigram model, it should be paded as \"I like NLP STOP\".\n",
    "\n",
    "1. Implement the ``pad_sents`` function.\n",
    "2. Pad ``train_sents``.\n",
    "3. Apply ``compute_ngram`` function to these padded sents.\n",
    "4. Implement ``ngram_prob`` function. Compute the probability for each n-gram in the variable **ngrams** according equations in **\"Parameter estimation\"**. List down the n-grams that have 0 probability. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'computer'], ['go', 'to'], ['have', 'had'], ['and', 'the'], ['can', 'sea'], ['a', 'number', 'of'], ['with', 'respect', 'to'], ['in', 'terms', 'of'], ['not', 'good', 'bad'], ['first', 'start', 'with']]\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "ngrams = list()\n",
    "with open('../data/ngram.txt','r') as f:\n",
    "    for line in f:\n",
    "        ngrams.append(line.strip('\\n').split())\n",
    "print(ngrams)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"<START>\"\n",
    "STOP = \"<STOP>\"\n",
    "###################################\n",
    "def pad_sents(sents, n):\n",
    "    \"\"\"\n",
    "    Pad the sents according to n.\n",
    "    params:\n",
    "        sents: list[list[str]] --- list of sentences.\n",
    "        n: int --- specify the padding type, 1-gram, 2-gram, or 3-gram.\n",
    "    return:\n",
    "        padded_sents: list[list[str]] --- list of padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sents = []\n",
    "    ### YOUR CODE HERE\n",
    "    for sentence in sents:\n",
    "        padded_sents.append([\"START\"] * np.max([0, n - 1]) + sentence + [\"STOP\"])\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return padded_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_sents = pad_sents(train_sents, 1)\n",
    "bi_sents = pad_sents(train_sents, 2)\n",
    "tri_sents = pad_sents(train_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_set, unigram_dict = compute_ngram(uni_sents, 1)\n",
    "bigram_set, bigram_dict = compute_ngram(bi_sents, 2)\n",
    "trigram_set, trigram_dict = compute_ngram(tri_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28910, 580115, 1356254)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unigram_set),len(bigram_set),len(trigram_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007146\n"
     ]
    }
   ],
   "source": [
    "num_words = sum([v for _,v in unigram_dict.items()])\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    \"\"\"\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        prob: float --- probability of the \"ngram\"\n",
    "    \"\"\"\n",
    "    prob = None\n",
    "    ### YOUR CODE HERE\n",
    "    if len(ngram) == 1:\n",
    "        prob = unigram_dic[tuple(ngram)] / num_words\n",
    "    elif len(ngram) == 2:\n",
    "        prob = bigram_dic.get(tuple(ngram), 0) / unigram_dic.get(tuple([ngram[0]]), 1)\n",
    "    elif len(ngram) == 3:\n",
    "        prob = trigram_dic.get(tuple(ngram), 0) / bigram_dic.get(\n",
    "            tuple([ngram[0], ngram[1]]), 1\n",
    "        )\n",
    "    else:\n",
    "        prob = \"Nan\"\n",
    "    ### END OF YOUR CODE\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.960235674499498e-05"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_prob(ngrams[0], num_words, unigram_dict, bigram_dict, trigram_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### List down the n-grams that have 0 probability. \n",
    "### YOUR CODE HERE\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kAezpJ9CdXd"
   },
   "source": [
    "### Question 3 [code]\n",
    "\n",
    "1. Implement ``add_k_smoothing_ngram`` function to estimate ngram probability with ``add-k`` smoothing technique.\n",
    "2. Implement ``interpolation_ngram`` function to estimate ngram probability with ``interpolation`` smoothing technique.\n",
    "3. Implement ``perplexity`` function to compute the perplexity of the corpus \"**valid_sents**\" according to \"**Perplexity**\" section. The computation of $p(X^{(j)})$ depends on the n-gram model you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wikitext-2/wiki.valid.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    valid_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    valid_sents = [s for s in valid_sents if len(s)>0 and s[0] != '=']\n",
    "\n",
    "uni_valid_sents = pad_sents(valid_sents, 1)\n",
    "bi_valid_sents = pad_sents(valid_sents, 2)\n",
    "tri_valid_sents = pad_sents(valid_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_k_smoothing_ngram(ngram, k, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        k: float \n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        s_prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    s_prob = None\n",
    "    V = len(unigram_dic)\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return s_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation_ngram(ngram, lam, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
    "    '''\n",
    "    params:\n",
    "        ngram: list[str] --- a list that represents n-gram\n",
    "        lam: list[float] --- a list of length 3.lam[0], lam[1] and lam[2] are correspondence to trigram, bigram and unigram,repectively.\n",
    "                             If len(ngram) == 1, lam[0]=lam[1]=0, lam[2]=1. If len(ngram) == 2, lam[0]=0. lam[0]+lam[1]+lam[2] = 1.\n",
    "        num_words: int --- total number of words\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "    return:\n",
    "        s_prob: float --- probability of the \"ngram\"\n",
    "    '''\n",
    "    s_prob = None\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return s_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "add_k_prob = add_k_smoothing_ngram(ngrams[5], 0.01, num_words, unigram_dict, bigram_dict, trigram_dict)\n",
    "interpolation_prob = interpolation_ngram(ngrams[5], [0.6,0.3,0.1], num_words, unigram_dict, bigram_dict, trigram_dict)\n",
    "print(ngrams[5])\n",
    "print(add_k_prob, interpolation_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(n, method, num_words, valid_sents, unigram_dic, bigram_dic, trigram_dic, k=0, lam=[0,0,1]):\n",
    "    '''\n",
    "    params:\n",
    "        n: int --- n-gram model you choose \n",
    "        method: int ---- method == 0, use add_k_smoothing; method != 0, use interpolation method.\n",
    "        num_words: int --- total number of words\n",
    "        valid_sents: list[list[str]] --- list of sentences\n",
    "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
    "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
    "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
    "        k: float --- The parameter of add_k_smoothing\n",
    "        lam: list[float] --- a list of length 3. The parameter of interpolation. \n",
    "   return:\n",
    "        ppl: float --- perplexity of valid_sents\n",
    "    '''\n",
    "    ppl = None\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity(1, 0, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict, k=0.1, lam=[0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 [code][written]\n",
    "1. Based on add-k smoothing method, try out different $k\\in [ 0.0001, 0.001, 0.01, 0.1, 0.5]$ and different n-gram model (unigram, bigram and trigram). Find the model and $k$ that gives the best perplexity on \"**valid_sents**\" (smaller is better).\n",
    "2. Based on interpolation method, try out different $\\lambda$ where $\\lambda_1 = \\lambda_2$ and $\\lambda_3\\in [0.1, 0.2, 0.4, 0.6, 0.8]$. Find the $\\lambda$ that gives the best perplexity on \"**valid_sents**\" (smaller is better).\n",
    "3. Based on the methods and parameters we provide, choose the method that peforms best on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [1,2,3]\n",
    "k = [0.0001, 0.001, 0.01, 0.1, 0.5]\n",
    "### YOUR CODE HERE (add-k smoothing method)\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_3 = [0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "### YOUR CODE HERE (interpolation method)\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the methods and parameters we provide, choose the method that peforms best on the validation data (**write your answer**): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3o_yjh9lCdXg"
   },
   "source": [
    "### Question 5 [code]\n",
    "\n",
    "Evaluate the perplexity of the test data **test_sents** based on the best model you choose in **Question 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7hGcXgCCdXg"
   },
   "outputs": [],
   "source": [
    "with open('data/wikitext-2/wiki.test.tokens', 'r', encoding='utf8') as f:\n",
    "    text = f.readlines()\n",
    "    test_sents = [line.lower().strip('\\n').split() for line in text]\n",
    "    test_sents = [s for s in test_sents if len(s)>0 and s[0] != '=']\n",
    "\n",
    "uni_test_sents = pad_sents(test_sents, 1)\n",
    "bi_test_sents = pad_sents(test_sents, 2)\n",
    "tri_test_sents = pad_sents(test_sents, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePSI8RDWCdXj"
   },
   "source": [
    "## Neural Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jkoTco_jCdXj"
   },
   "source": [
    "<img src=\"images/LM.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "We will create a LSTM language model as shown in figure and train it on the Wikitext-2 dataset. \n",
    "The data generators (train\\_iter, valid\\_iter, test\\_iter) have been provided. \n",
    "The word embeddings together with the parameters in the LSTM model will be learned from scratch.\n",
    "\n",
    "[Pytorch](https://pytorch.org/tutorials/) and [torchtext](https://torchtext.readthedocs.io/en/latest/index.html#) are required in this part. Do not make any changes to the provided code unless you are requested to do so. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 [code]\n",
    "- Implement the ``__init__`` function in ``LangModel`` class. *Note: the code implementation should allow switching between unidirectional LSTM and bidirectional LSTM easily*\n",
    "- Implement the ``forward`` function in ``LangModel`` class.\n",
    "- Complete the training code in train function and the testing code in test function.\n",
    "- Train two models - **Unidirectional LSTM** and **Bidirectional LSTM**. Compute the perplexity of the test data \"test_iter\" using the trained models. The test perplexity of both trained models should be below 150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note: Make sure that \"torchtext <= 0.11\", as newer version might have torchtext.legacy removed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tF2GCRdFCdXk",
    "outputId": "7e2ee790-c23f-48dd-e4a2-763293db1880"
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchtext.legacy.datasets import WikiText2\n",
    "from torch import nn, optim\n",
    "from torchtext.legacy import data\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "torch.manual_seed(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lt6JBdSnCdXn",
    "outputId": "7d11709e-1821-473e-aece-5add82208067"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    '''Tokenize a string to words'''\n",
    "    return word_tokenize(text)\n",
    "\n",
    "START = '<START>'\n",
    "STOP = '<STOP>'\n",
    "#Load and split data into three parts\n",
    "TEXT = data.Field(lower=True, tokenize=tokenizer, init_token=START, eos_token=STOP)\n",
    "train, valid, test = WikiText2.splits(TEXT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHwu4VMVCdXq",
    "outputId": "4fb02a02-8f07-4239-b1f0-45234cda2b2d"
   },
   "outputs": [],
   "source": [
    "#Build a vocabulary from the train dataset\n",
    "TEXT.build_vocab(train)\n",
    "print('Vocabulary size:', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ua9e-OBMCdXs"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# the length of a text feeding to the RNN layer\n",
    "BPTT_LEN = 32           \n",
    "# train, validation, test data\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
    "                                                                batch_size=BATCH_SIZE,\n",
    "                                                                bptt_len=BPTT_LEN,\n",
    "                                                                repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6_Cd8shXCdXy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate a batch of train data\n",
    "batch = next(iter(train_iter))\n",
    "text, target = batch.text, batch.target\n",
    "print('Size of text tensor',text.size())\n",
    "print('Size of target tensor',target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangModel(nn.Module):\n",
    "    def __init__(self, lang_config):\n",
    "        super(LangModel, self).__init__()\n",
    "        self.vocab_size = lang_config['vocab_size']\n",
    "        self.emb_size = lang_config['emb_size']\n",
    "        self.hidden_size = lang_config['hidden_size']\n",
    "        self.num_layer = lang_config['num_layer']\n",
    "        \n",
    "        self.embedding = None\n",
    "        self.lstm = None\n",
    "        self.linear = None\n",
    "        \n",
    "        ### TODO: \n",
    "        ###    1. Initialize 'self.embedding' with nn.Embedding function and 2 variables we have initialized for you\n",
    "        ###    2. Initialize 'self.lstm' with nn.LSTM function and 4 variables we have initialized for you \n",
    "        ###    3. Initialize 'self.linear' with nn.Linear function and 2 variables we have initialized for you\n",
    "        ### Reference:\n",
    "        ###        https://pytorch.org/docs/stable/nn.html\n",
    "        \n",
    "        ### YOUR CODE HERE (3 lines)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "        \n",
    "    def forward(self, batch_sents, hidden=None):\n",
    "        '''\n",
    "        params:\n",
    "            batch_sents: torch.LongTensor of shape (sequence_len, batch_size)\n",
    "        return:\n",
    "            normalized_score: torch.FloatTensor of shape (sequence_len, batch_size, vocab_size)\n",
    "        '''\n",
    "        normalized_score = None\n",
    "        hidden = hidden\n",
    "        ### TODO:\n",
    "        ###      1. Feed the batch_sents to self.embedding  \n",
    "        ###      2. Feed the embeddings to self.lstm. Remember to pass \"hidden\" into self.lstm, even if it is None. But we will \n",
    "        ###         use \"hidden\" when implementing greedy search.\n",
    "        ###      3. Apply linear transformation to the output of self.lstm\n",
    "        ###      4. Apply 'F.log_softmax' to the output of linear transformation\n",
    "        ###\n",
    "        ### YOUR CODE HERE (4 lines)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "        return normalized_score, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs):\n",
    "    for n in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        target_num = 0\n",
    "        model.train()\n",
    "        for batch in train_iter:\n",
    "            \n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            loss = None\n",
    "            \n",
    "            ### we don't consider \"hidden\" here. So according to the default setting, \"hidden\" will be None\n",
    "            ### YOU CODE HERE (~5 lines)\n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "            ##########################################\n",
    "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        train_loss /= target_num\n",
    "\n",
    "        # monitor the loss of all the predictions\n",
    "        val_loss = 0\n",
    "        target_num = 0\n",
    "        model.eval()\n",
    "        for batch in valid_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "            \n",
    "            prediction,_ = model(text)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "            \n",
    "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "        val_loss /= target_num\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, vocab_size, criterion, test_iter):\n",
    "    '''\n",
    "    params: \n",
    "        model: LSTM model\n",
    "        test_iter: test data\n",
    "    return:\n",
    "        ppl: perplexity \n",
    "    '''\n",
    "    ppl = None\n",
    "    test_loss = 0\n",
    "    target_num = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            text, targets = batch.text.to(device), batch.target.to(device)\n",
    "\n",
    "            prediction,_ = model(text)\n",
    "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
    "\n",
    "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
    "            target_num += targets.size(0) * targets.size(1)\n",
    "\n",
    "        test_loss /= target_num\n",
    "        \n",
    "        ### Compute perplexity according to \"test_loss\"\n",
    "        ### Hint: Consider how the loss is computed.\n",
    "        ### YOUR CODE HERE(1 line)\n",
    "\n",
    "        ### END OF YOUR CODE\n",
    "        return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "criterion = nn.NLLLoss(reduction='mean')\n",
    "optimizer = optim.Adam(LM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size':vocab_size,\n",
    "    'emb_size':128,\n",
    "    'hidden_size':128,\n",
    "    'num_layer':1,\n",
    "    'bidirectional': False\n",
    "}\n",
    "\n",
    "LM = LangModel(config)\n",
    "LM = LM.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(LM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(LM, vocab_size, criterion, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'vocab_size':vocab_size,\n",
    "    'emb_size':128,\n",
    "    'hidden_size':128,\n",
    "    'num_layer':1,\n",
    "    'bidirectional': True\n",
    "}\n",
    "\n",
    "biLSTM = LangModel(config)\n",
    "biLSTM = biLSTM.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(biLSTM, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(biLSTM, vocab_size, criterion, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 [code][written]\n",
    "<img src=\"images/greedy.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
    "\n",
    "When we use trained language model to generate a sentence given a start token, we can choose ``greedy search``.\n",
    "\n",
    "As shown above, ``greedy search`` algorithm will pick the token which has the highest probability and feed it to the language model as input in the next time step. The model will generate ``max_len`` number of tokens at most.\n",
    "\n",
    "- Implement ``word_greedy_search``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_greedy_search(model, start_token, max_len):\n",
    "    '''\n",
    "    param:\n",
    "        model: nn.Module --- language model\n",
    "        start_token: str --- e.g. 'he'\n",
    "        max_len: int --- max number of tokens generated\n",
    "    return:\n",
    "        strings: list[str] --- list of tokens, e.g., ['he', 'was', 'a', 'member', 'of',...]\n",
    "    '''\n",
    "    model.eval()\n",
    "    ID = TEXT.vocab.stoi[start_token]\n",
    "    strings = [start_token]\n",
    "    hidden = None\n",
    "    \n",
    "    ### You may find TEXT.vocab.itos useful.\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END OF YOUR CODE \n",
    "    print(strings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_greedy_search(LM, 'he', 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Question: Based on your understanding, can we use the **Bidirectional LSTM** for this language generation (decoding) task? Explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**write your explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 [code][written]\n",
    "- We will use the hidden vectors (the working memory) of LSTM as the contextual embeddings. Implement ``contextual_embedding`` function.\n",
    "- Use the ``contextual_embedding`` function to get the contextual embeddings of the word \"sink\" in four sequences \"wood does not sink in water\", \"a small water leak will sink the ship\", \"there are plates in the kitchen sink\" and \"the kitchen sink was full of dirty dishes\". Then calculate the cosine similarity of \"sink\" from each pair of sequences. Assume that $\\boldsymbol{w}_1$ and $\\boldsymbol{w}_2$ are embeddings of \"sink\" in sequences \"wood does not sink in water\" and \"a small water leak will sink the ship\" respectively. The cosine similarity can be calculated as \n",
    "\n",
    "\\begin{align}\n",
    "similarity = cos(\\theta) = \\frac{\\boldsymbol{w}^{\\rm T}_1\\boldsymbol{w}_2}{||\\boldsymbol{w}_1||_2||\\boldsymbol{w}_2||_2}\n",
    "\\end{align}\n",
    "Give the explanation of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_embedding(model, sentence):\n",
    "    '''\n",
    "    params: \n",
    "        model: nn.Module --- language model\n",
    "        sentence -- list[str]: list of tokens, e.g., ['I', 'am',...]\n",
    "    return:\n",
    "        embeddings -- numpy array of shape (length of sentence, word embedding size)\n",
    "    '''\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    \n",
    "    ### YOUR CODE HERE \n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink_seq1 = \"wood does not sink in water\"\n",
    "sink_seq2 = \"a small water leak will sink the ship\"\n",
    "sink_seq3 = \"there are plates in the kitchen sink\"\n",
    "sink_seq4 = \"the kitchen sink was full of dirty dishes\"\n",
    "\n",
    "### YOUR CODE HERE \n",
    "\n",
    "### END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***write your explanation:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Question: Based on your understanding, can we use the **Bidirectional LSTM** for this contextual embedding task? Explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**write your explanation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cernoBq6CdX9"
   },
   "source": [
    "### Requirements:\n",
    "- This is an individual report.\n",
    "- Complete the code using Python.\n",
    "- List students with whom you have discussed if there are any.\n",
    "- Follow the honor code strictly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bDOUuC7-CdX-"
   },
   "source": [
    "### Free GPU Resources\n",
    "We suggest that you run neural language models on machines with GPU(s). Google provides the free online platform [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use as common packages have been  pre-installed. Google users can have access to a Tesla T4 GPU (approximately 15G memory). Note that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM.\n",
    "\n",
    "It is convenient to upload local Jupyter Notebook files and data to Colab, please refer to the [tutorial](https://colab.research.google.com/notebooks/io.ipynb). \n",
    "\n",
    "In addition, Microsoft also provides the online platform [Azure Notebooks](https://notebooks.azure.com/help/introduction) for research of data science and machine learning, there are free trials for new users with credits."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mini project.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b2a4c03de4558ef5a607eb077c8ee8a6fae43eb49b3aff39fb8abe80b4d90c52"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlp-t8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
